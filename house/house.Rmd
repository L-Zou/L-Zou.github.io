---
title: "House Prices: Advanced Regression Techniques: SVM and Random Forests"
author: "Lily Zou"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(e1071)
library(randomForest)
library(corrplot)
library(pROC)
library(ROCR)
```

## 1 Summary

The purpose of this project is to predict the final price of a residential home in Ames, Iowa using the 79 explanatory variables provided (MSSubClass: The building class; MSZoning: The general zoning classification; LotFrontage: Linear feet of street connected to property; LotArea: Lot size in square feet; Street: Type of road access; Alley: Type of alley access; etc.). 

The datasets used are available at https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data.

## 2 Data  

### 2.1 Read Data

```{r 2.1, warning = FALSE}
train = read.csv("train.csv", header = T)
test = read.csv("test.csv", header = T)
```

The description of each variable can be viewed in the "data_description.txt" file present in the same folder as this file. 

### 2.2 Data Preparation

When importing the datasets into R, R automatically assigns each variable a variable type (continuous, catgeorical, etc.). 

However some types were incorrectly assigned. For example, the variable "MSSubClass" was meant to be a categorical variable. However, R interpreted it to be an integer variable. 

Here we will change the variable type of each explanatory variable into their proper variable type.

```{r 2.2a}
## train 
train$MSSubClass = as.factor(as.character(train$MSSubClass))
train$OverallQual = as.factor(as.character(train$OverallQual))
train$OverallCond = as.factor(as.character(train$OverallCond))
train$MoSold = as.factor(as.character(train$MoSold))
train$YrSold = as.factor(as.character(train$YrSold))

## test
test$MSSubClass = as.factor(as.character(test$MSSubClass))
test$OverallQual = as.factor(as.character(test$OverallQual))
test$OverallCond = as.factor(as.character(test$OverallCond))
test$MoSold = as.factor(as.character(test$MoSold))
test$YrSold = as.factor(as.character(test$YrSold))
```

### 2.3 Missing Values

Note that there are several variables with the value NA: 

```{r 2.3a}
mvtrain = sort(sapply(train, function(x) sum(is.na(x))))

mvtest = sort(sapply(test, function(x) sum(is.na(x))))

mvtrain[mvtrain > 0]
mvtest[mvtest > 0]
```

For the following variables we make the assumption that a value of NA means that a house did not have this particular characteristic (since these characteristics are not necessities for a house): 

* Alley: Type of alley access

* Masonry veneer characterists:
    - MasVnrType: Masonry veneer type
    - MasVnrArea: Masonry veneer area in square feet
    
* Basement characteristics:
    - BsmtQual: Height of the basement
    - BsmtCond: General condition of the basement
    - BsmtExposure: Walkout or garden level basement walls
    - BsmtFinType1: Quality of basement finished area
    - BsmtFinType2: Type 1 finished square feet
    - BsmtFinSF1: Quality of second finished area (if present)
    - BsmtFinSF2 Type 2 finished square feet
    - BsmtUnfSF: Unfinished square feet of basement area
    - TotalBsmtSF: Total square feet of basement area
    - BsmtFullBath: Basement full bathrooms
    - BsmtHalfBath: Basement half bathrooms
    
* FireplaceQu: Fireplace quality

* Garage characteristics: 
    - GarageType: Garage location
    - GarageYrBlt: Year garage was built
    - GarageFinish: Interior finish of the garage
    - GarageQual: Garage quality
    - GarageCond: Garage condition
    
* PoolQC: Pool quality

* Fence: Fence quality

* MiscFeature: (Miscellaneous feature not covered in other categories)

Consequently we will replace NA with 0 for integer variables and "None" for factor variables. 

```{r 2.3b}
replaceNone0 = c('Alley', 'MasVnrType', 'MasVnrArea', 'BsmtQual',
                 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 
                 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'FireplaceQu', 
                 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath',
                 'GarageType', 'GarageYrBlt', 'GarageFinish', 
                 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 
                 'MiscFeature')

for (var in replaceNone0){
  if (class(train[,var]) == "factor"){
      levels(train[,var]) = c(levels(train[,var]), "None")
      train[[var]][is.na(train[[var]])] = "None"
  }
  else if (class(train[,var]) == "integer"){
      train[[var]][is.na(train[[var]])] = 0
  }
}

for (var in replaceNone0){
  if (class(test[,var]) == "factor"){
      levels(test[,var]) = c(levels(test[,var]), "None")
      test[[var]][is.na(test[[var]])] = "None"
  }
  else if (class(test[,var]) == "integer"){
      test[[var]][is.na(test[[var]])] = 0
  }
}

mvtrain = sort(sapply(train, function(x) sum(is.na(x))))

mvtest = sort(sapply(test, function(x) sum(is.na(x))))

mvtrain[mvtrain > 0]
mvtest[mvtest > 0]
```

For the other variables, since only a max of 17\% of the data is missing, we replace factor variables with the most common category and for integer variables we use the mean of all the other values. The other variables are as follows: 

* MSZoning: The general zoning classification
* Utilities: Type of utilities available
* Exterior characteristics
    - Exterior1st: Exterior covering on house
    - Exterior2nd: Exterior covering on house (if more than one material) 
* KitchenQual: Kitchen quality
* Functional: Home functionality rating
* SaleType: Type of sale
* Electrical: Electrical system
* LotFrontage: Linear feet of street connected to property
* Garage characteristics
    - GarageCars: Size of garage in car capacity
    - GarageArea: Size of garage in square feet

```{r 2.3c}
calculate_mode = function(x) {
  uniq <- unique(na.omit(x))
  uniq[which.max(tabulate(match(x, uniq)))]
}

replace = c('MSZoning', 'Utilities', 'Exterior1st', 'Exterior2nd',
            'KitchenQual', 'Functional', 'SaleType', 'Electrical',
            'LotFrontage', 'GarageCars', 'GarageArea')

for (var in replace){
  if (class(train[,var]) == "factor"){
      train[[var]][is.na(train[[var]])] =
        calculate_mode(train[,var])
  }
  else if (class(train[,var]) == "integer"){
      train[[var]][is.na(train[[var]])] =
        round(mean(na.omit(train[,var])))
      train[,var] = as.integer(train[,var])
  }
}

for (var in replace){
  if (class(test[,var]) == "factor"){
      test[[var]][is.na(test[[var]])] =
        calculate_mode(test[,var])
  }
  else if (class(test[,var]) == "integer"){
      test[[var]][is.na(test[[var]])] =
        round(mean(na.omit(test[,var])))
      test[,var] = as.integer(test[,var])
  }
}

mvtrain = sort(sapply(train, function(x) sum(is.na(x))))

mvtest = sort(sapply(test, function(x) sum(is.na(x))))

mvtrain[mvtrain > 0]
mvtest[mvtest > 0]
```

Additionally we will ensure factor levels are consistent across the training and test set. 

```{r 2.3d}
ncol = 80
for (i in 1:ncol){
  if (class(train[,i]) == "factor"){
    if (length(levels(test[,i])) > length(levels(train[,i]))){
      levels(train[,i]) = levels(test[,i])
    }
    else if (length(levels(test[,i])) < length(levels(train[,i]))){
      levels(test[,i]) = levels(train[,i])
    }
  }
} 
```

### 2.4 Data Exploration

```{r 2.4a}
contvar = c()
for (i in 2:ncol){
  if ((class(train[,i]) == "integer")|| (class(train[,i]) == "numeric")){
    contvar = append(contvar, i)
  }
} 

corr = cor(train[,contvar])

corrplot(corr, method = "color")
```

Only a couple pairs of variables have high correlation: GarageArea (Size of garage in square feet) and Garage Cars (Size of garage in car capacity), X1stFlrSF (First Floor square feet) and TotalBsmtSF (Total square feet of basement area), TotRmsAbvGrd (Total rooms above grade) and GrLivArea (Above grade (ground) living area square feet). However due to the nature of these variables none of these correlations are surprising. None of these variables should be removed as these variables still convey important information regarding the housing structure. 

## 3 Model

### 3.1 Random Forest

Fitting a random forest model with all the covariates: 

```{r 3.1a}
rffit = randomForest(SalePrice ~ ., data = train[2:81],
                     importance=TRUE)
print(rffit)
head(round(importance(rffit), 2))
```

Fitting a random forest model using covariates whose importance is greater than the mean importance of all the covariates: 

```{r 3.1b}
(round(importance(rffit), 2)[,'IncNodePurity'] > 
   mean(round(importance(rffit), 2)[,2]))[(round(importance(rffit), 2)[,'IncNodePurity'] > 
                                                                     mean(round(importance(rffit), 2)[,2])) == TRUE]

rffitm = randomForest(SalePrice ~ LotArea+
                        Neighborhood+
                        OverallQual+
                        YearBuilt+
                        ExterQual +
                        BsmtQual  +
                        BsmtFinSF1  +
                        TotalBsmtSF  +
                        X1stFlrSF  +
                        X2ndFlrSF  +
                        GrLivArea  +
                        KitchenQual + 
                        GarageCars  +
                        GarageArea, 
                      data = train[2:81],
                      importance=TRUE)
print(rffitm)
head(round(importance(rffitm), 2))
```

Fitting a random forest model using covariates whose importance is in the top 10th percentile: 

```{r 3.1c}
(round(importance(rffit), 2)[,'IncNodePurity'] > 
   quantile(round(importance(rffit), 2)[,'IncNodePurity'], 0.90))[(round(importance(rffit), 2)[,'IncNodePurity'] > 
                                                                     quantile(round(importance(rffit), 2)[,'IncNodePurity'],
                                                                              0.90)) == TRUE]

rffitper = randomForest(SalePrice ~ Neighborhood+
                       OverallQual+
                       ExterQual+
                       TotalBsmtSF+
                       X1stFlrSF+
                       GrLivArea+
                       GarageCars+
                       GarageArea , 
                     data = train[2:81],
                     importance=TRUE)
print(rffitper)
head(round(importance(rffitper), 2))
```

### 3.2 SVM

Fitting a model using SVM with all the covariates: 

```{r 3.2a}
svmfit = svm(SalePrice ~ ., data = train[2:81])
print(svmfit)
```

## 4 Evaluation

To evaluate the accuracy of the predictions we will use the Root-Mean-Squared-Error (RMSE) with the logarithm of the predicted values and the logarithm of the observed values (Root-Mean-Squared-Logarithmic-Error or RMSLE). The reasoning being that by taking the log, errors in predicting expensive houses and cheap houses, will affect the results equally. The RMSE is a measure of the standard deviation of the residuals, therefore, the smaller the RMSE the better the model fits the data. 

```{r 4rmsle}
rmsle = function(pred = NULL, act = NULL){
  val = sqrt((1/length(pred))* sum((log(pred+1)- log(act + 1))^2))
  return(val)
}
```

### 4.1 Random Forest

For the random forest model with all the covariates we have a RMSLE of 0.06438. 

```{r 4.1a}
rffit_pred = predict(rffit, train)

rmsle(rffit_pred, train$SalePrice)
```

For the random forest model with only the covariates whose importance is greater than the mean importance of all the covariates we have a RMSLE of 0.07054. 

```{r 4.1b}
rffitm_pred = predict(rffitm, train)

rmsle(rffitm_pred, train$SalePrice)
```

For the random forest model with only the covariates whose importance is in the top 10th percentile  we have a RMSLE of 0.08569. 

```{r 4.1c}
rffitper_pred = predict(rffitper, train)

rmsle(rffitper_pred, train$SalePrice)
```

Therefore on the training dataset, the random forest model based on all the covariates performs better. However on the test dataset: 

```{r 4.1test}
rffit_price = predict(rffit, test[2:80])
rffitm_price = predict(rffitm, test[2:80])
rffitper_price = predict(rffitper, test[2:80])

df1 = data.frame(Id = test$Id, SalePrice = rffit_price)
write.csv(df1,"prediction1.csv", row.names = FALSE)

df2 = data.frame(Id = test$Id, SalePrice = rffitm_price)
write.csv(df2,"prediction2.csv", row.names = FALSE)

df3 = data.frame(Id = test$Id, SalePrice = rffitper_price)
write.csv(df3,"prediction3.csv", row.names = FALSE)
```

Submitting the predictions to Kaggle reveals that: 

* Random forest model with all the covariates RMSLE: 0.15888
* Random forest model with only the covariates whose importance is greater than the mean importance of all the covariates RMSLE: 0.15198
* Random forest model with only the covariates whose importance is in the top 10th percentile RMSLE: 0.16636.

Therefore, on the test dataset, since the random forest model based on the mean importance performs the best out of all the other random forest models since it has the lowest RMSLE. 

### 4.2 SVM

```{r 4.2a}
svmfit_pred = predict(svmfit, train)

rmsle(svmfit_pred, train$SalePrice)


price_svm = predict(svmfit, test)
dfSVM = data.frame(Id = test$Id, SalePrice = price_svm)
write.csv(dfSVM,"predictionSVM.csv", row.names = FALSE)
```

The RMSLE of the SVM model on the training dataset is 0.11552. 

On the test dataset the RMSLE is 0.14583.

## 4.3 SVM and Random Forest

Between the mean based random forest model and the SVM model, the SVM model has the lower RMSLE. 

As an exercise we will now assess the predictive power the SVM model at predicting the whether a house's sale price will be above or below the median housing price (based on the training data set). 

To do so we will create a new variable SalePriceMedian where SalePriceMean is 1 if the house's sale price is median and 0 if it is below. 

```{r 4.3a}
medhouseprice = median(train$SalePrice)

converttoSalePriceMedian = function(SalesPrice = NULL){
  val = c()
  for (i in 1:length(SalesPrice)){
    if (SalesPrice[i] > medhouseprice){
      val = append(val, 1)
    }
    else{
      val = append(val, 0)
    }
  }
  return(val)
}

train$SalePriceMedian = as.integer(converttoSalePriceMedian(train$SalePrice))
svmfit_pred_SalePriceMedian =
  as.integer(converttoSalePriceMedian(svmfit_pred))

svmfit_pred = predict(svmfit, train, type = "decision")
pr = prediction(svmfit_pred, train$SalePriceMedian)
perf = performance(pr, measure="tpr",x.measure="fpr")
plot(perf)
lines(x = c(0,1), y = c(0,1),col="blue")

auc = performance(pr, measure = 'auc')
auc@y.values[[1]]
```

The AUC of ROC curve for the SVM model is 0.98688, indicating that the probability of the model ranking a house with a price above the median price above a house with a price below the median price is 99\%. 

